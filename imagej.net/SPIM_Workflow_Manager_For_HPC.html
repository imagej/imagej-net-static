<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>SPIM Workflow Manager For HPC - ImageJ</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"SPIM_Workflow_Manager_For_HPC","wgTitle":"SPIM Workflow Manager For HPC","wgCurRevisionId":42626,"wgRevisionId":42626,"wgArticleId":10531,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"SPIM_Workflow_Manager_For_HPC","wgRelevantArticleId":10531,"wgRequestId":"1c20828342c2652339d477e7","wgIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgPreferredVariant":"en","fancytree_path":"/extensions/TreeAndMenu/fancytree"});mw.loader.state({"site.styles":"ready","noscript":"ready","user.styles":"ready","user.cssprefs":"ready","user":"ready","user.options":"loading","user.tokens":"loading","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","skins.erudite":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1ku9xth",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["mediawiki.page.startup"]);});</script>
<link rel="stylesheet" href="load.php%3Fdebug=false&amp;lang=en&amp;modules=mediawiki.legacy.commonPrint%252Cshared%257Cmediawiki.sectionAnchor%257Cskins.erudite&amp;only=styles&amp;skin=erudite.css"/>
<script async="" src="load.php%3Fdebug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=erudite"></script>
<link rel="stylesheet" href="extensions/TreeAndMenu/fancytree/fancytree.css"/><link rel="stylesheet" href="extensions/TreeAndMenu/suckerfish/suckerfish.css"/>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="load.php%3Fdebug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=erudite.css"/>
<meta name="generator" content="MediaWiki 1.28.0"/>
<meta name="description" content="SPIM Workflow Manager for HPC is a Fiji plugin developed at IT4Innovations, Ostrava, Czech Republic. The plugin enables biology users to upload data to the remote cluster, monitor computation progress, and examine and download results via the local Fiji installation."/>
<link rel="shortcut icon" href="skins/ij2.ico"/>
	<meta property="og:type" content="article"/>

	<meta property="og:site_name" content="ImageJ"/>

	<meta property="og:title" content="SPIM Workflow Manager For HPC"/>

	<meta property="og:description" content="SPIM Workflow Manager for HPC is a Fiji plugin developed at IT4Innovations, Ostrava, Czech Republic. The plugin enables biology users to upload data to the remote cluster, monitor computation progress, and examine and download results via the local Fiji installation."/>


<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-SPIM_Workflow_Manager_For_HPC rootpage-SPIM_Workflow_Manager_For_HPC skin-erudite action-view">
		<div class="mw-jump">
			<a href="SPIM_Workflow_Manager_For_HPC.html#bodyContent">Skip to content</a>, 			<a href="SPIM_Workflow_Manager_For_HPC.html#search">Skip to search</a>
		</div>

		<div id="top-wrap" role="banner">
			<h1><a href="Welcome" title="ImageJ" rel="home">ImageJ</a></h1>
			<div id="tagline">From ImageJ</div>

			<a id="menubutton" href="SPIM_Workflow_Manager_For_HPC.html#menu">Menu</a>
			<div id="nav" role="navigation">
			<ul id='menu'>
<li id="menu-item-n-About"><a href="ImageJ">About</a></li>
<li id="menu-item-n-Downloads"><a href="Downloads">Downloads</a></li>
<li id="menu-item-n-Learn"><a href="Learn">Learn</a></li>
<li id="menu-item-n-Develop"><a href="Development">Develop</a></li>
<li id="menu-item-n-News"><a href="News">News</a></li>
<li id="menu-item-n-Events"><a href="Events">Events</a></li>
<li id="menu-item-n-Help"><a href="Help">Help</a></li>
</ul>
			</div>
		</div>

		<div id="mw-js-message"></div>
		
		<div id="main" role="main">

			<div id="bodyContent">
				<h1>SPIM Workflow Manager For HPC</h1>
				
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><table class="infobox" cellspacing="5" style="max-width: 31em; font-size: 90%; text-align: left; float: right; border: 1px solid #a0a0a0;">
<tr>
<th colspan="2" style="text-align: center; font-size: 130%;"> <div style="float: left"></div>SPIM Workflow Manager for HPC (Fiji)<div style="clear: left;"></div>
</th></tr>
<tr>
<th> Author
</th>
<td> Jan Kožusznik, Petr Bainar, Jana Klímová, Michal Krumnikl, Pavel Moravec, Pavel Tomancak
</td></tr>




<tr>
<th> Maintainer
</th>
<td> jan.kozusznik@vsb.cz
</td></tr>


<tr>
<th> Source
</th>
<td> <a rel="nofollow" class="external text" href="https://github.com/fiji-hpc/hpc-workflow-manager/">on github</a>
</td></tr>
<tr>
<th> Initial release
</th>
<td> August 2018
</td></tr>
<tr>
<th> Latest version
</th>
<td> March 2019
</td></tr>

<tr>
<th> Category
</th>
<td> <a href="./Category:Transform" title="Category:Transform">Transform</a>, <a href="./Category:Registration" title="Category:Registration">Registration</a>, <a href="./Category:Deconvolution" title="Category:Deconvolution">Deconvolution</a>
</td></tr>

</table>
<p><br />
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="SPIM_Workflow_Manager_For_HPC.html#About_the_plugin"><span class="tocnumber">1</span> <span class="toctext">About the plugin</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="SPIM_Workflow_Manager_For_HPC.html#Background"><span class="tocnumber">2</span> <span class="toctext">Background</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="SPIM_Workflow_Manager_For_HPC.html#Description"><span class="tocnumber">3</span> <span class="toctext">Description</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="SPIM_Workflow_Manager_For_HPC.html#SPIM_data_processing_pipeline"><span class="tocnumber">3.1</span> <span class="toctext">SPIM data processing pipeline</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="SPIM_Workflow_Manager_For_HPC.html#HEAppE_middleware"><span class="tocnumber">3.2</span> <span class="toctext">HEAppE middleware</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="SPIM_Workflow_Manager_For_HPC.html#Installation"><span class="tocnumber">4</span> <span class="toctext">Installation</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="SPIM_Workflow_Manager_For_HPC.html#Usage"><span class="tocnumber">5</span> <span class="toctext">Usage</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="SPIM_Workflow_Manager_For_HPC.html#HPC_Cluster"><span class="tocnumber">6</span> <span class="toctext">HPC Cluster</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="SPIM_Workflow_Manager_For_HPC.html#Citation"><span class="tocnumber">7</span> <span class="toctext">Citation</span></a></li>
</ul>
</div>

<h1><span class="mw-headline" id="About_the_plugin">About the plugin</span></h1>
<p>SPIM Workflow Manager for HPC is a Fiji plugin developed at IT4Innovations, Ostrava, Czech Republic. The plugin enables biology users to upload data to the remote cluster, monitor computation progress, and examine and download results via the local Fiji installation.
</p>
<h1><span class="mw-headline" id="Background">Background</span></h1>
<p>Imaging techniques have emerged as a crucial means of understanding the structure and function of living organisms in primary research, as well as medical diagnostics.
In order to maximize information gain, achieving as high spatial and temporal resolution as practically possible is desired.
However, long-term time-lapse recordings at the single-cell level produce vast amounts of multidimensional image data, which cannot be processed on a personal computer in a timely manner, therefore requiring utilization of high-performance computing (HPC) clusters.
For example, processing a 2.2 TB dataset of drosophila embryonic development, taking a week on a single computer, was brought down to 13 hours by employing an HPC cluster supporting parallel execution of individual tasks <a rel="nofollow" class="external text" href="Automated_workflow_for_parallel_Multiview_Reconstruction">[automated workflow</a>].
Unfortunately, life scientists often lack access to such infrastructure.
</p><p>Addressing this issue is particularly challenging as Fiji is an extraordinarily extensible platform and new plugins emerge incessantly.
So far, plugin developers have typically implemented task parallelization within a particular plugin, but no universal approach has yet been incorporated into the SciJava architecture.
Here we propose the concept of integrating parallelization support into one of the SciJava libraries, thereby enabling developers to access remote resources (e.g., remote HPC infrastructure) and delegate plugin-specific tasks to its compute nodes.
As the cluster-specific details are hidden in respective interface implementations, the plugins can remain extensible and technology-agnostic.
In addition, the proposed solution is highly scalable, meaning that any additional resources can be efficiently utilized.
</p>
<h1><span class="mw-headline" id="Description">Description</span></h1>
<h2><span class="mw-headline" id="SPIM_data_processing_pipeline">SPIM data processing pipeline</span></h2>
<p>SPIM ("Selective/Single Plane Illumination Microscopy") typically images living biological samples from multiple angles (views) collecting several 3D image stacks to cover the entire biological specimen. The 3D image stacks, representing one time point in a long-term time-lapse acquisition, need to be registered to each other which is typically achieved using fluorescent beads as fiduciary markers.
After the registration, the individual views within one time point need to be combined into a single output image either by content-based fusion or multi-view deconvolution <a rel="nofollow" class="external text" href="Multiview-Reconstruction">[Multiview-reconstruction</a>]. The living specimen can move during acquisition, necessitating an intermediate step of time-lapse registration. Whereas parallel processing of individual time points has proven to be beneficial, the time-lapse registration takes only a few seconds and can therefore be performed on a single computing node without the need for parallelization.
</p><p>The sheer amount of the SPIM data requires conversion from raw microscopy data to Hierarchical Data Format (HDF5) for efficient input/output access and visualization in Fiji's <a rel="nofollow" class="external text" href="BigDataViewer#Publication">BigDataViewer</a> (BDV). BDV uses an XML file to store experiment metadata (i.e. number of angles, time points, channels etc.). Although the conversion to HDF5 is a parallelizable procedure, further updating the XML file downstream in the pipeline is not; and per-time point XML files have to be created and then merged after completion of the registration and fusion steps. Consequently, the parallel processing of individual time points on an HPC resource (conversion to HDF5, registration, fusion and deconvolution) is interrupted by non-parallelizable steps (time-lapse registration and XML merging).
</p><p>Pipeline input parameters are entered by a user into a <i>config.yaml</i> configuration file. In the first step, the .czi raw data are concurrently resaved into the HDF5 container in parallel on the cluster. Similarly, the individual time points are registered in parallel using fluorescent beads as fiduciary markers on the cluster. Subsequently, a non-parallel job executed by <i>Snakemake</i> consolidate the registration XML files into a single one, followed by time-lapse registration using the beads segmented during the spatial registration step. After this, the pipeline diverge into either parallel content-based fusion or parallel multi-view deconvolution. To achieve this divergence in practice, the <i>Snakemake</i> pipeline is launched from the Fiji plugin as two separate jobs using two different <i>config.yaml</i> files set to execute content-based fusion and deconvolution respectively. In the final stage of the pipeline, the fusion/deconvolution output is saved into a new HDF5 container. Figure below shows results of registration, fusion and deconvolution in different time points.
<a href="./File:Drosophila.PNG" class="image"><img alt="Drosophila.PNG" src="_images/5/55/Drosophila.PNG" width="800" height="238" /></a>
</p>
<h2><span class="mw-headline" id="HEAppE_middleware">HEAppE middleware</span></h2>
<p>Accessing a remote HPC cluster is often burdened by administrative overhead due to more or less complex security policies enforced by HPC centers. This barrier can be substantially lowered by employing a middleware tool based on the HPC-as-a-Service concept. To provide this simple and intuitive access to the supercomputing infrastructure an in-house application framework called High-End Application Execution <a rel="nofollow" class="external text" href="http://heappe.eu">(HEAppE)</a> Middleware has been developed. This middleware provides HPC capabilities to the users and third-party applications without the need to manage the running jobs from the command-line interface of the HPC scheduler on the cluster.
</p><p>To facilitate access to HPC from the Fiji environment, we utilize the in-house HEAppE Middleware framework allowing end users to access an HPC system through web services and remotely execute pre-defined tasks. Furthermore, HEAppE is designed to be universal and applicable to various HPC architectures. HEAppE also provides the mapping between the external users and internal cluster service accounts that are being used for the actual job submission to the cluster. It simplifies the access to the computation resources from the security and administrative point of view. For security purposes, users are permitted to run only a pre-prepared set of so-called command templates. Each command template defines an arbitrary script or an executable file which is to be run on the cluster, a set of input parameters modifiable at runtime, any dependencies or third-party software it might require, and the type of queue that should be used for the processing.
</p><p>We developed a Fiji plugin underlain by HEAppE, which enables users to steer workflows running on a remote HPC resource. As a representative workflow we use a <i>Snakemake</i> based SPIM data processing pipeline operating on large image datasets. The <i>Snakemake</i> workflow engine resolves dependencies between subsequent steps and executes in parallel any tasks appearing to be independent, such as processing of individual time points of a time-lapse acquisition.
</p>
<h1><span class="mw-headline" id="Installation">Installation</span></h1>
<p>After you install and launch <a rel="nofollow" class="external text" href="Fiji/Downloads">Fiji</a>, go to <span><em><span style="border-bottom:1px dotted #ccc;"> Help </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Update... </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Manage update sites</span></em></span>, tick <i>P2E-IT4Innovations</i> and close the window. Then click <i>Apply changes</i> and restart Fiji.
</p>
<h1><span class="mw-headline" id="Usage">Usage</span></h1>
<p>Now you should see the plugin under <span><em><span style="border-bottom:1px dotted #ccc;"> Plugins </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Multiview Reconstruction </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> SPIM Workflow Manager for HPC</span></em></span>. Upon plugin invocation from the application menu, you are prompted for HEAppE credentials, e-mail address and specifying your working directory. Following a successful login, the main window containing all jobs arranged in a table is displayed. In this context, the term <i>job</i> is used for a single pipeline run with specified parameters. The plugin actively inquires information on the created jobs from HEAppE and updates the table as appropriate.
<a href="./File:Flowchart.PNG" class="image"><img alt="Flowchart.PNG" src="_images/3/3c/Flowchart.PNG" width="400" height="477" /></a>
</p><p>For creating a new job, right click in the main window and choose <i>Create a new job</i>. A window with input and output data location will pop up. You have the option to use demonstration data on the Salamon cluster or specify your own input data location. Eventually you may choose your working directory (specified during login) as both your input and output data location. Once a new job is configured, you are able to upload your own data (if you chose this option in the previous step) by right clicking on the job line and choosing <i>Upload data</i>. When <i>Done</i> appears in the <i>Upload</i> column, you can start the job by <span><em><span style="border-bottom:1px dotted #ccc;"> right click </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Start job</span></em></span>. Status of your job changes to <i>Queued</i>, then to <i>Running</i> and finally to <i>Finished</i> when your pipeline finishes successfully.
</p><p>The plugin provides a wizard allowing you to set up a configuration file <i>config.yaml</i>, which effectively characterizes the dataset and defines settings for individual workflow tasks. The plugin supports uploading local input image data to the remote HPC resource, providing information on the progress and estimated remaining time.
</p><p>Once a job execution is selected by you, the configuration file is sent to the cluster via HEAppE, which is responsible for the job life cycle from this point on. You can display a detailed progress dashboard showing current states of all individual computational tasks for the selected job as well as output logs useful for debugging.
<a href="./File:Ui_screens.PNG" class="image"><img alt="Ui screens.PNG" src="_images/4/4d/Ui_screens.PNG" width="1200" height="268" /></a>
</p><p>Following a successfully finished pipeline, you can interactively examine the processed SPIM image data using the <a rel="nofollow" class="external text" href="BigDataServer">BigDataServer</a> as well as download resultant data and a summary file containing key information about the performed job. Importantly, you can edit the corresponding local configuration file in a common text editor, and restart an interrupted, finished, or failed job.
</p>
<h1><span class="mw-headline" id="HPC_Cluster">HPC Cluster</span></h1>
<p>Execution of the <i>Snakemake</i> pipeline from the implemented Fiji plugin was tested on the <a rel="nofollow" class="external text" href="https://docs.it4i.cz/salomon/introduction/">Salomon</a> supercomputer, at IT4Innovations in Ostrava, Czech Republic, which consists of 1 008 compute nodes, each of which is equipped with 2x12-core Intel Haswell processors and 128 GB RAM, providing a total of 24 192 compute cores of x86-64 architecture and 129 TB RAM. Furthermore, 432 nodes are accelerated by two Intel Xeon Phi 7110P accelerators with 16 GB RAM each, providing additional 52 704 cores and 15 TB RAM. The total theoretical peak performance reaches 2 000 TFLOPS. The system runs a Red Hat Linux. 
</p><p>Using the developed plugin, we executed the pipeline on the Salomon supercomputer. As the test data set we used 90 time-point SPIM acquisition of a <i>Drosophila melanogaster</i> embryo expressing FlyFos fluorescent GFP fusion reporter for the <i>nrv2</i> gene.
The embryo was imaged with Lightsheet Z.1 SPIM microscope (Carl Zeiss Microscopy) from 5 views every 15 minutes from cellular blastoderm stage until late stages of fruitfly embryogenesis. The dataset consisted of 170 GB of image data.
</p><p>The data transfer and pipeline execution on Salomon using 90 nodes took 9 hours 37 minutes. For comparison, processing of the same dataset on a common workstation took 23 hours 56 minutes. The results show that despite the data transfer overhead, a significant speedup of SPIM image analysis has been achieved by employing HPC resources.
</p>
<h1><span class="mw-headline" id="Citation">Citation</span></h1>
<p>Please note that the plugin SPIM Workflow Manager for HPC available through Fiji is based on a publication. If you use it successfully for your research please be so kind to cite our work:
</p><p>Jan Kožusznik, Petr Bainar, Jana Klímová, Michal Krumnikl, Pavel Moravec, Václav Svatoň, Pavel Tomančák; SPIM Workflow Manager for HPC, <i>Bioinformatics</i>, btz140, <a rel="nofollow" class="external free" href="https://doi.org/10.1093/bioinformatics/btz140">https://doi.org/10.1093/bioinformatics/btz140</a>
</p>
<!-- 
NewPP limit report
Cached time: 20200713073305
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.044 seconds
Real time usage: 0.047 seconds
Preprocessor visited node count: 289/1000000
Preprocessor generated node count: 1197/1000000
Post‐expand include size: 2519/2097152 bytes
Template argument size: 893/2097152 bytes
Highest expansion depth: 6/40
Expensive parser function count: 0/3
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%   10.470      1 - -total
 61.34%    6.422      3 - Template:Bc
 35.25%    3.691      1 - Template:Infobox
 19.45%    2.036      5 - Template:Arrow
-->
</div><div class="printfooter">
Retrieved from "<a dir="ltr" href="index.php?title=SPIM_Workflow_Manager_For_HPC&amp;oldid=42626">http://imagej.net/index.php?title=SPIM_Workflow_Manager_For_HPC&amp;oldid=42626</a>"</div>
							</div>

			<div id="footer">
				<p> This page was last modified on 20 February 2020, at 09:08.</p><ul><li><a href="./ImageJ:Privacy_policy" title="ImageJ:Privacy policy">Privacy policy</a></li><li><a href="./ImageJ:About" class="mw-redirect" title="ImageJ:About">About ImageJ</a></li><li><a href="Imprint" title="Imprint">Imprint</a></li></ul>			</div>

			<div id="catlinks" class="catlinks catlinks-allhidden" data-mw="interface"></div>		</div>

		<div id="bottom-wrap">
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.loader.load(["ext.fancytree","ext.suckerfish","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","ext.SimpleTooltip"]);});</script><script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":239});});</script>
		</body>
		</html>
		