<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Principles - ImageJ</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Principles","wgTitle":"Principles","wgCurRevisionId":41307,"wgRevisionId":41307,"wgArticleId":774,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Tutorials"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Principles","wgRelevantArticleId":774,"wgRequestId":"9360e26e47425bbaea97058e","wgIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgPreferredVariant":"en","fancytree_path":"/extensions/TreeAndMenu/fancytree"});mw.loader.state({"site.styles":"ready","noscript":"ready","user.styles":"ready","user.cssprefs":"ready","user":"ready","user.options":"loading","user.tokens":"loading","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","skins.erudite":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1ku9xth",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["mediawiki.page.startup"]);});</script>
<link rel="stylesheet" href="load.php%3Fdebug=false&amp;lang=en&amp;modules=mediawiki.legacy.commonPrint%252Cshared%257Cmediawiki.sectionAnchor%257Cskins.erudite&amp;only=styles&amp;skin=erudite.css"/>
<script async="" src="load.php%3Fdebug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=erudite"></script>
<link rel="stylesheet" href="extensions/TreeAndMenu/fancytree/fancytree.css"/><link rel="stylesheet" href="extensions/TreeAndMenu/suckerfish/suckerfish.css"/>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="load.php%3Fdebug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=erudite.css"/>
<meta name="generator" content="MediaWiki 1.28.0"/>
<meta name="description" content="The page is a collection of principles for the entire image analysis process, from acquisition to processing to analysis."/>
<link rel="shortcut icon" href="skins/ij2.ico"/>

<script type="text/javascript" src="extensions/SyntaxHighlighter/syntaxhighlighter/scripts/shCore.js"></script>
<script type="text/javascript" src="extensions/SyntaxHighlighter/syntaxhighlighter/scripts/shBrushJScript.js"></script>
<script type="text/javascript">
SyntaxHighlighter.all();
</script>
<link rel="stylesheet" type="text/css" media="screen" href="extensions/SyntaxHighlighter/syntaxhighlighter/styles/shCoreMinit.css" />

	<meta property="og:type" content="article"/>

	<meta property="og:site_name" content="ImageJ"/>

	<meta property="og:title" content="Principles"/>

	<meta property="og:description" content="The page is a collection of principles for the entire image analysis process, from acquisition to processing to analysis."/>


<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Principles rootpage-Principles skin-erudite action-view">
		<div class="mw-jump">
			<a href="Principles.html#bodyContent">Skip to content</a>, 			<a href="Principles.html#search">Skip to search</a>
		</div>

		<div id="top-wrap" role="banner">
			<h1><a href="Welcome" title="ImageJ" rel="home">ImageJ</a></h1>
			<div id="tagline">From ImageJ</div>

			<a id="menubutton" href="Principles.html#menu">Menu</a>
			<div id="nav" role="navigation">
			<ul id='menu'>
<li id="menu-item-n-About"><a href="ImageJ">About</a></li>
<li id="menu-item-n-Downloads"><a href="Downloads">Downloads</a></li>
<li id="menu-item-n-Learn"><a href="Learn">Learn</a></li>
<li id="menu-item-n-Develop"><a href="Development">Develop</a></li>
<li id="menu-item-n-News"><a href="News">News</a></li>
<li id="menu-item-n-Events"><a href="Events">Events</a></li>
<li id="menu-item-n-Help"><a href="Help">Help</a></li>
</ul>
			</div>
		</div>

		<div id="mw-js-message"></div>
		
		<div id="main" role="main">
			<div id="nav-meta">
			<span id="ca-nstab-main" class="selected"><a href="Principles" title="View the content page [c]" accesskey="c">Page</a></span><span class="meta-sep">|</span><span id="ca-talk" class="new"><a href="index.php?title=Talk:Principles&amp;action=edit&amp;redlink=1" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Discussion</a></span><span class="meta-sep">|</span><span id="ca-viewsource"><a href="index.php?title=Principles&amp;action=edit" title="This page is protected.&#10;You can view its source [e]" accesskey="e">View source</a></span><span class="meta-sep">|</span><span id="ca-history"><a href="index.php?title=Principles&amp;action=history" title="Past revisions of this page [h]" accesskey="h">History</a></span><span class="meta-sep">|</span>			</div>

			<div id="bodyContent">
				<h1>Principles</h1>
				
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><p>The page is a collection of principles for the entire image analysis process, from acquisition to processing to analysis.
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="Principles.html#Image_acquisition_principles"><span class="tocnumber">1</span> <span class="toctext">Image acquisition principles</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="Principles.html#Introduction"><span class="tocnumber">1.1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="Principles.html#Use_sufficient_spatial_resolution"><span class="tocnumber">1.2</span> <span class="toctext">Use sufficient spatial resolution</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="Principles.html#Avoid_lossy_compression"><span class="tocnumber">1.3</span> <span class="toctext">Avoid lossy compression</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="Principles.html#Illuminate_as_evenly_as_possible"><span class="tocnumber">1.4</span> <span class="toctext">Illuminate as evenly as possible</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="Principles.html#Avoid_overlapping_objects"><span class="tocnumber">1.5</span> <span class="toctext">Avoid overlapping objects</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="Principles.html#Naming_schemes"><span class="tocnumber">1.6</span> <span class="toctext">Naming schemes</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="Principles.html#Sample_preparation"><span class="tocnumber">1.7</span> <span class="toctext">Sample preparation</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="Principles.html#Image_processing_principles"><span class="tocnumber">2</span> <span class="toctext">Image processing principles</span></a>
<ul>
<li class="toclevel-2 tocsection-10"><a href="Principles.html#Introduction_2"><span class="tocnumber">2.1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="Principles.html#What_are_pixel_values.3F"><span class="tocnumber">2.2</span> <span class="toctext">What are pixel values?</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="Principles.html#Pixels_are_not_little_squares"><span class="tocnumber">2.3</span> <span class="toctext">Pixels are not little squares</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="Principles.html#Why_.28lossy.29_JPEGs_should_not_be_used_in_imaging"><span class="tocnumber">2.4</span> <span class="toctext">Why (lossy) JPEGs should not be used in imaging</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="Principles.html#Considerations_during_image_segmentation_.28binarization.29"><span class="tocnumber">2.5</span> <span class="toctext">Considerations during image segmentation (binarization)</span></a>
<ul>
<li class="toclevel-3 tocsection-15"><a href="Principles.html#What_is_binarization_and_what_is_it_good_for.3F"><span class="tocnumber">2.5.1</span> <span class="toctext">What is binarization and what is it good for?</span></a></li>
<li class="toclevel-3 tocsection-16"><a href="Principles.html#Why_not_simply_choose_a_manual_threshold.3F"><span class="tocnumber">2.5.2</span> <span class="toctext">Why not simply choose a manual threshold?</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="Principles.html#Which_automatic_methods_do_exist_for_binarization.3F"><span class="tocnumber">2.5.3</span> <span class="toctext">Which automatic methods do exist for binarization?</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="Principles.html#What_else_is_critical_during_binarization_and_further_object_analysis.3F"><span class="tocnumber">2.5.4</span> <span class="toctext">What else is critical during binarization and further object analysis?</span></a>
<ul>
<li class="toclevel-4 tocsection-19"><a href="Principles.html#Image_acquisition"><span class="tocnumber">2.5.4.1</span> <span class="toctext">Image acquisition</span></a></li>
<li class="toclevel-4 tocsection-20"><a href="Principles.html#Pre-processing"><span class="tocnumber">2.5.4.2</span> <span class="toctext">Pre-processing</span></a></li>
<li class="toclevel-4 tocsection-21"><a href="Principles.html#Post-processing"><span class="tocnumber">2.5.4.3</span> <span class="toctext">Post-processing</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-22"><a href="Principles.html#Segmented_ROIs_for_additional_processing"><span class="tocnumber">2.5.5</span> <span class="toctext">Segmented ROIs for additional processing</span></a></li>
<li class="toclevel-3 tocsection-23"><a href="Principles.html#How_do_I_check_the_quality_of_a_binarization.3F"><span class="tocnumber">2.5.6</span> <span class="toctext">How do I check the quality of a binarization?</span></a></li>
<li class="toclevel-3 tocsection-24"><a href="Principles.html#What_should_I_do_with_true_color_images_.28e.g._histological_staining.29_with_different_colors_and_not_a_single_intensity_scale.3F"><span class="tocnumber">2.5.7</span> <span class="toctext">What should I do with true color images (e.g. histological staining) with different colors and not a single intensity scale?</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-25"><a href="Principles.html#Image_analysis_principles"><span class="tocnumber">3</span> <span class="toctext">Image analysis principles</span></a>
<ul>
<li class="toclevel-2 tocsection-26"><a href="Principles.html#Introduction_3"><span class="tocnumber">3.1</span> <span class="toctext">Introduction</span></a></li>
</ul>
</li>
</ul>
</div>

<h1><span class="mw-headline" id="Image_acquisition_principles">Image acquisition principles</span></h1>
<h2><span class="mw-headline" id="Introduction">Introduction</span></h2>
<p>Not all data is created equal and thus the analysis of certain images can be easily automated, while others pose a bigger challenge.
</p><p>The goal of this section is to collect information on image acquisition principles that ease the automation of image analysis.
</p>
<h2><span class="mw-headline" id="Use_sufficient_spatial_resolution">Use sufficient spatial resolution</span></h2>
<p>Spatial resolution refers to the number (or density, if you prefer) of samples in the image. Digital detectors such as cameras and PMTs can produce sample matrices ranging from 256 x 256 pixels or fewer, up to 128 megapixels or more. As a rule of thumb, more samples is better. Spatial resolution can always be downsampled after the fact—but never upsampled. Furthermore, if your objects of interest are described by too few pixels, the error of many statistical computations will be prohibitively high, and some forms of analyses will not be possible at all.
</p>
<h2><span class="mw-headline" id="Avoid_lossy_compression">Avoid lossy compression</span></h2>
<p>Original data should be saved in a way that preserves the exact sample values. Do not store raw image data in file formats such as JPEG which use lossy compression. See <a href="Principles#Why_.28lossy.29_JPEGs_should_not_be_used_in_imaging" title="Principles">Why (lossy) JPEGs should not be used in imaging</a> below for details.
</p>
<h2><span class="mw-headline" id="Illuminate_as_evenly_as_possible">Illuminate as evenly as possible</span></h2>
<p>Many forms of imaging require some form of illumination. You should ensure that this illumination is as evenly distributed as possible, rather than attempting to correct for it after acquisition. If you <i>must</i> tolerate an uneven illumination for some reason, try to acquire a background image so that you can use a background subtraction—but there may still be issues such as reflection artifacts.
</p>
<h2><span class="mw-headline" id="Avoid_overlapping_objects">Avoid overlapping objects</span></h2>
<p>In many cases, it is not possible to avoid objects which overlap. But they are harder to analyze and measure, since many algorithms have difficulty distinguishing between objects. It may be possible to mitigate these difficulties by preparing the environment somehow—e.g., staining cell membranes with a fluorescent dye.
</p>
<h2><span class="mw-headline" id="Naming_schemes">Naming schemes</span></h2>
<p>Effective naming schemes are easy to read by both humans and computers. The following examples give an overview over the principles of creating good naming schemes:
</p>
<ul><li> Example A:</li></ul>
<h2><span class="mw-headline" id="Sample_preparation">Sample preparation</span></h2>
<p>In certain cases (Examples ...) it is very helpful to add markers to the slides.
</p>
<h1><span class="mw-headline" id="Image_processing_principles">Image processing principles</span></h1>
<h2><span class="mw-headline" id="Introduction_2">Introduction</span></h2>
<p>In scientific image processing and image analysis, an image is something different than a regular digital photograph of a beautiful scene you shot during your latest vacation.
</p><p>In the context of science, digital images are samples of information, sampled at vertex points of <i>n</i>-dimensional grids.
</p>
<h2><span class="mw-headline" id="What_are_pixel_values.3F">What are pixel values?</span></h2>
<p>Human visual perception is very good at certain tasks, such as contrast correction and detecting subtle differences in bright colors, but notoriously bad with other things, such as discerning dark colors, or classifying colors without appropriate reference.  Our brains process and filter information, so that we perceive visually only after we 1) "see" light information that enters our eyes and interacts with the environment of the eye and eventually individual cells in our retinas, and 2) process the signal in the context of our brains. It is therefore very important to keep in mind that the pixel values in digital images are numbers, not subjective experiences of color. These numbers represent how light or some other type of signal entered the instrument we are using and interacted with its environment and eventually triggered sensors in the instrument that further processed the information into a digital output.
</p><p>For example, when you record an image using a light gathering device such as a confocal microscope, the values you get at a certain coordinate or pixel are not color values, but relate to photon counts. This is an essential point to start with for understanding this topic.
</p>
<h2><span class="mw-headline" id="Pixels_are_not_little_squares">Pixels are not little squares</span></h2>
<p>And voxels are not cubes! See the <a rel="nofollow" class="external text" href="http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf">whitepaper by Alvy Ray Smith</a>.
</p>
<h2><span class="mw-headline" id="Why_.28lossy.29_JPEGs_should_not_be_used_in_imaging">Why (lossy) JPEGs should not be used in imaging</span></h2>
<p>JPEG stands for Joint Photographic Experts Group who were the creators of a commonly used method of lossy compression for photographic images. This format is commonly used in web pages and supported by the vast majority of digital photographic cameras and image scanners because it can store images in relatively small files at the expense of image quality. There are also loss-less JPEG modes, but these in general are not widely implemented and chances are that most of the images are of the lossy type.
</p><p>"<b>Lossy compression</b>" means that in the process of file size reduction certain amount of image information is discarded. In the case of JPEGs, this might not be readily obvious to an observer, but it will be important for image processing purposes.
</p><p>"<b>Loss-less or non-lossy compression</b>" means that the file size is reduced, but the data stored is exactly the same as in the original.
</p><p>In JPEG lossy compression, therefore, the stored image is not the same as the original, and hence <b>not suitable</b> for doing serious imaging work. While JPEG compression throws away information that the eye cannot detect easily, this results in considerable image artifacts (variable "blockiness" of the image in groups of 8x8 pixels). Any attempts to process or quantify those images will be affected in uncontrollable ways by the presence of the artifacts. This is particularly obvious in the hue channel of JPEG compressed colour images when converted to HSB colour space.
</p><p>Example: a section of the famous  <a rel="nofollow" class="external text" href="http://sampl.ece.ohio-state.edu/data/stills/color/mandrill.png">Mandrill</a> image. From left to right, you see the original (with an 8-bit colormap), the hue channel of the original, and the hue channel after saving as a JPEG with ImageJ's default options -- note in particular the vertical and horizontal artifacts:
</p>
<div class="MediaTransformError" style="width: 248px; height: 0px; display:inline-block;">Error creating thumbnail: Unable to save thumbnail to destination</div><div class="MediaTransformError" style="width: 248px; height: 0px; display:inline-block;">Error creating thumbnail: Unable to save thumbnail to destination</div><div class="MediaTransformError" style="width: 248px; height: 0px; display:inline-block;">Error creating thumbnail: Unable to save thumbnail to destination</div>
<p>While most digital cameras save in JPEG format by default, it is very likely that they also support some non-lossy format (such as TIFF or a custom RAW format). Use those formats instead.
A format called JPEG2000 supported by various slide scanners and used in "virtual slide" products was created to improve image quality and compression rates, however both lossy and non-lossy versions the JPEG2000 format exist. Use only the non-lossy formats.
</p><p><b>Multichannel images</b> in particular are harmed by the JPEG format: since the multiple channels are misinterpreted as red, green and blue (while most channels integrate more than just one wavelength), JPEG will shift the colors imperceptibly to improve compression. In the worst case, this can lead to <a href="Colocalization_Analysis" title="Colocalization Analysis">colocalization</a> where there was none.
</p><p><b>TIFF</b> or <b>PNG</b> formats available in ImageJ/Fiji are non-lossy formats. TIFF preserves any calibrations applied to your images, but images are not compressed. PNG files are smaller as non-lossy compression is applied but they do not store calibration data.
</p><p>Once an image has been saved as compressed JPEG there is no way of reverting to the original, therefore an old JPEG-compressed image saved again as TIFF or PNG still contains all the original JPEG compression artifacts.
</p><p>Below is an ImageJ macro which demonstrates the issue. It requires the <a href="Glasbey" title="Glasbey">Glasbey</a> LUT, part of the <a href="Fiji" title="Fiji">Fiji</a> distribution of ImageJ.
</p>
<pre class="brush:javascript">
// load Boats
run("Boats (356K)");
run("Out [-]");
rename("Original");

// convert to JPEG
run("Duplicate...", " ");
run("Out [-]");
run("Save As JPEG... [j]", "jpeg=85");
run("Revert");
rename("JPEG");

// compute the difference
imageCalculator("Subtract create 32-bit", "Original","JPEG");
run("Out [-]");
rename("Difference");

// display windows side by side
run("Tile");

// highlight artifacts using Glasbey LUT
selectWindow("Original");
run("glasbey");
selectWindow("JPEG");
run("glasbey");
selectWindow("Difference");
</pre>
<h2><span class="mw-headline" id="Considerations_during_image_segmentation_.28binarization.29">Considerations during image segmentation (binarization)</span></h2>
<h3><span class="mw-headline" id="What_is_binarization_and_what_is_it_good_for.3F">What is binarization and what is it good for?</span></h3>
<p>As the word already suggests, with a binarization you divide your image into two (not necessarily connected) parts usually referred to as foreground and background. This is the simplest method of image segmentation. "Simple" does not refer to easily achieving good results; it is, however, often perceived by the user as being easy to execute. There are also methods which group pixels of an image into different classes. Those ones are mostly based on statistical approaches. Some are implemented in Fiji such as the "<a href="Statistical_Region_Merging" title="Statistical Region Merging">Statistical Region Merging</a>" plugin or the "<a href="Linear_Kuwahara" title="Linear Kuwahara">Linear Kuwahara filter</a>". There exist even more sophisticated classification methods often based on machine learning and training by the user with a set of representative test images. While image binarization can also be achieved with methods including statistics the simplest would be to set one or two (upper and lower) cut-off value(s) separating specific pixel intensities from each other. This is often referred to as a threshold (value).
Separating image features and objects by their intensity is often suitable if the intensity is the parameter which directly relates to the spatial characteristics of the objects and defines those. This is for example often the case in fluorescence microscopy with clearly stained cellular structures and low background staining. If other parameters define the structures outline or area, a simple threshold does often not lead to satisfying results or even fails completely doing the job.
Many users are aware of a method which allows them to manually define such a threshold value but this comes with several limitations...
</p>
<h3><span class="mw-headline" id="Why_not_simply_choose_a_manual_threshold.3F">Why not simply choose a manual threshold?</span></h3>
<p>"I usually define a manual threshold to extract my objects..., is this ok?"
As a recommendation... Whenever possible, try to avoid manual thresholding!!!
</p><p><b>Manual methods have several limitations:</b>
</p>
<ol type="a">
<li>no/low <a href="Reproducibility" class="mw-redirect" title="Reproducibility">reproducibility</a> </li>
<li>high user bias</li>
<li>tedious and time-consuming fiddling-around finding an "appropriate" cut-off value</li>
<li>incompatibility with automatic processing</li>
<li>high intra- and inter-user variability</li>
</ol>
<p>You might think of determining a fixed manual threshold and then applying the determined cut-off values to all your images. In this case, you face the decision of choosing one from 256 values. This would at least meet the criterion to treat equally all the images that you need to compare, but will not make you happy in the end. This is because there is a certain variability in the images of one single experiment and generally even higher variability between the images of replicated experiments, such that one fixed value will not extract similar features from different images. It is certainly a goal of digital image processing to establish protocols that eliminate variability, but currently a lofty goal, and even the most diligent of efforts to replicate circumstances generally fail to produce sets of images that can be thresholded with one fixed value. That is, it is not a realistic expectation in most fields.
</p><p><b>So what is the alternative?</b>
</p><p>An alternative goal is to develop segmentation methods that overcome the variability. Indeed, segmentation algorithms have been a goal in biology and other areas for decades. The multifarious algorithms that have been developed are based on a vast array of different image properties (see below) but they all have in common that the determination of the threshold is based on image-intrinsic properties and not on subjective, real-time user decisions. 
</p><p><b>Inherently Subjective Components of Automated Segmentation</b>
</p><p>Being based on image-intrinsic properties does not mean automated segmentation methods are strictly objective and bias free. Obviously, since the user has a choice of algorithms, the final decision on which algorithm to apply under specific conditions or a specific experiment is necessarily subjective. This is itself a significant issue - for one thing, with respect to the decision posed earlier in this section about choosing from 256 possible cut-off values, the number of available algorithms is also high and continues to grow. 
</p>
<div id="trainingbias">Training Bias</div>Moreover, an equally momentous problem is that segmentation methods are generally inherently biased in that they are trained in the first place against an initial human perception of what the final information extracted from an image should be. This is a reasonable basis of course, and its benefit is easily understood in the example of face recognition software trained to threshold faces based on human input on what constitutes a face. But the principle itself introduces bias. To understand negative consequences of this sort of bias, imagine a scenario in pathology, in which automated segmentation is developed to yield a certain result based on current expert pathologist experience, yet completely eliminates a cell feature that is later learned to be a critical indicator. 
<p>Automated segmentation is biased in other regards. Consider, for instance, the difference between global and local thresholds in binary segmentation. During <a rel="nofollow" class="external text" href="Thresholding#Global_thresholding">global thresholding</a>, the image as a whole is taken to determine the cut-off value and this value is applied to each pixel in the image. During <a rel="nofollow" class="external text" href="Thresholding#Local_thresholding">local thresholding</a>, in contrast, as the name suggests, values are determined in a local environment (often defined by the radius of a circular neighborhood) and the local cut-off values are locally applied to the individual pixels of the image. Thus, darker areas in an image might be extractable comparably as well as very bright areas, but this would not be possible with a global threshold, where specifically very dark areas shift into the background or might be under-extracted.
</p><p><b>Benefits of Automated Segmentation</b>
</p><p>Their limitations notwithstanding, the <b>advantages of automatic binarization methods</b> are:
</p>
<ol type="a">
<li>they are fully <a href="Reproducible" class="mw-redirect" title="Reproducible">reproducible</a> (on the same image they will always lead to the same binarization result)</li>
<li>they introduce no user bias during thresholding (this is not related to bias associated with the choice of specific automatic algorithm, which does exist)</li>
<li>they use objectively determined cut-off values to minimize some variability in images being compared (e.g., algorithms that react to each image's histogram often extract features deemed superior to features extracted using fixed values; but again, see the <a href="Principles.html#trainingbias">above note</a> on a priori training of segmentation methods)</li>
<li>they reduce preprocessing because they are easy applied to image stacks with variability between individual image histograms or in the complete stack histogram</li>
<li>they are fast (no fiddling) and can be automated (e.g., in macros, plugins, batch jobs, etc.)</li>
</ol>
<p>In ImageJ and Fiji, there are so far 16 <a href="Auto_Threshold" title="Auto Threshold">Global Auto Thresholds</a> and 9 <a href="Auto_Local_Threshold" title="Auto Local Threshold">Auto Local Thresholds</a> implemented (by Gabriel Landini). These are very flexible and practical and can be integrated in macros and plugins, and do an excellent job for many intensity based images.
</p><p><b>Is there one superior automatic algorithm?</b>
</p><p>For all of the reasons outlined above, NO! But this is not really a problem. It is akin to asking the question "Is there one superior food?". As is the case with biological protocols in general, most algorithms are developed serving a specific purpose or solving a specific extraction problem. Thus, performance is relative and depends on the image content and quality, and the intended use of the pattern extracted. This last statement can be understood in terms of a single image from which an experimenter may want to extract overall cell shape in one investigation but nuclear texture in another. Every basic method needs to be tested to determine if any of its implementations do a good job for every single new question to be answered. In this regard, there are many more algorithms published and otherwise being developed, and you might want to think about implementing one in an ImageJ plugin yourself and providing it to the community&#160;:-)
</p>
<h3><span class="mw-headline" id="Which_automatic_methods_do_exist_for_binarization.3F">Which automatic methods do exist for binarization?</span></h3>
<p>Generally, there are different groups of algorithms for image binarization.
(The following classification of methods is taken from Sezgin and Sankur, Survey over image thresholding techniques and quantitative performance evaluation, Journal of Electronic Imaging 13(1), 146–165 (January 2004).)
</p>
<ol><li> Histogram shape-based methods, where, for example, the peaks, valleys and curvatures of the smoothed histogram are analyzed.</li>
<li> Clustering-based methods, where the gray-level samples are clustered in two parts as background and foreground (object), or alternately are modelled as a mixture of two Gaussians.</li>
<li> Entropy-based methods result in algorithms that use the entropy of the foreground and background regions, the cross-entropy between the original and binarized image, etc.</li>
<li> Object attribute-based methods search a measure of similarity between the gray-level and the binarized images, such as fuzzy shape similarity, edge coincidence, etc.</li>
<li> The spatial methods use higher-order probability distribution and/or correlation between pixels </li>
<li> Local methods adapt the threshold value on each pixel to the local image characteristics.</li></ol>
<p>This list might not be comprehensive but gives a good idea about the extensive possibilities available.
</p>
<h3><span class="mw-headline" id="What_else_is_critical_during_binarization_and_further_object_analysis.3F">What else is critical during binarization and further object analysis?</span></h3>
<h4><span class="mw-headline" id="Image_acquisition">Image acquisition</span></h4>
<p>It is indispensable that during imaging the field of view be equally lit, and that this is checked and adjusted if necessary. Besides other disadvantages (e.g., no reliable intensity measurements), unequal lighting profoundly influences the outcome of thresholding methods, especially global ones. Furthermore, a higher signal-to-noise ratio, in e.g., fluorescence based images, will positively influence extraction by thresholding.
</p>
<h4><span class="mw-headline" id="Pre-processing">Pre-processing</span></h4>
<p>Since the basis of any cut-off value during thresholding is pixel values, any change in those after image acquisition will also influence the final binarization result. Thus, the user needs to apply wisely methods that adjust brightness and contrast or any image filters, to not degrade but instead enhance the features of interest, fostering the binarization process. It may not be feasible to apply the same process to all images. In general, all pre-processing should be quantitated and recorded to ensure work is <a href="Reproducible" class="mw-redirect" title="Reproducible">reproducible</a>. 
</p>
<h4><span class="mw-headline" id="Post-processing">Post-processing</span></h4>
<p>After being binarized through any thresholding method, an image may require additional binary operations to make the final pattern useable. These include erosion, dilation, opening, and closing (all under <span><em><span style="border-bottom:1px dotted #ccc;"> Process </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Binary</span></em></span>), image filters (under <span><em><span style="border-bottom:1px dotted #ccc;"> Process </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Filters</span></em></span>), and image combinations by boolean operations (e.g. <span><em><span style="border-bottom:1px dotted #ccc;"> Process </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Image Calculator</span></em></span>). Here the user needs to pay attention to recording all parameters and events and not alter the extraction notably, lest they reduce the <a href="Reproducibility" class="mw-redirect" title="Reproducibility">reproducibility</a> and overall quality of the segmentation. Post-processing binary operations might be necessary to correct further measurements of area or object counts. Internal holes, for example, may need to be closed (<span><em><span style="border-bottom:1px dotted #ccc;"> Process </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Binary </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Fill Holes</span></em></span>) to extract the correct area of particles (this can also be achieved directly during the measurement when using &gt;<span><em><span style="border-bottom:1px dotted #ccc;"> Analyze </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Analyze Particles...</span></em></span>.). Watershed (or related) separation techniques may be necessary when close particles fuse to form clumps or aggregates.
</p>
<h3><span class="mw-headline" id="Segmented_ROIs_for_additional_processing">Segmented ROIs for additional processing</span></h3>
<p>Once an object is binarized, it can be converted to an ROI automatically and the ROI reapplied to the original image for verification as well as processing of the original image based on the ROI. This is used, for instance, to identify, separate, and analyze features of overlapping cells. The method for creating the ROIs from a binary image is <span><em><span style="border-bottom:1px dotted #ccc;"> Edit </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Selection </span>&#160;&#8250; <span style="border-bottom:1px dotted #ccc;"> Create Selection</span></em></span>. This method is very useful when used with the RoiManager and in macros or plugins for automating tasks.
</p>
<h3><span class="mw-headline" id="How_do_I_check_the_quality_of_a_binarization.3F">How do I check the quality of a binarization?</span></h3>
<p>Several publications deal with methods to check the quality of image segmentation. Many methods refer to a comparison to a so called "ground truth" in the first place. For a user doing a binary segmentation, this is basically a user-created binarization of an example image to test the extraction "quality". Similarly, when a method is being developed, benchmark images are created in a variety of ways including by taking photos of idealized objects and manually thresholding or tracing overlays to produce a desirable result or by generating computer simulations to form the standard against which the process will be assessed. Depending on the method, the original structures could include a range from simple forms to complex biological or synthetic structures of all sizes and shapes. Ideally, independent comparisons to different image-intrinsic measures (e.g., object edges, known angles or ratios between structural components, numbers of particular structural components, exclusion of known artifacts, entropy, etc.) should also be included when verifying the quality of a binarization. Of course, even for a simple round ball, this whole process has to be seen critically, because the accuracy of the result is necessarily defined by a biased "ground truth" in part defined by what the desired features to be extracted are. Thus, a researcher generally provides some degree of verification, explains its rationale, and acknowledges its limitations when commenting on the quality of a binarization. 
</p><p>A related issue is training the user. The "<a href="BioVoxxel_Toolbox#Threshold_Check" title="BioVoxxel Toolbox">Threshold Check</a>", <a rel="nofollow" class="external autonumber" href="http://www.cscjournals.org/manuscript/Journals/IJIP/volume8/Issue2/IJIP-829.pdf">[1]</a> is a trial to make it easier for the untrained user, to decide for one or the other threshold. The semi-quantitative evaluation therein is also biased but might facilitate a rather unbiased comparison of different extraction methods (based on a user-chosen and thus biased reference value).
</p>
<h3><span class="mw-headline" id="What_should_I_do_with_true_color_images_.28e.g._histological_staining.29_with_different_colors_and_not_a_single_intensity_scale.3F">What should I do with true color images (e.g. histological staining) with different colors and not a single intensity scale?</span></h3>
<p>If converting to grayscale damages or distorts the information desired from a pattern, many different possibilities exist:
</p>
<ol><li> One option, related to the manual thresholding methods mentioned above, is not ideal but has particular uses. It can be combined partially with automatic intensity thresholding and is mentioned here out of completeness since it might lead to good results (e.g., for some histological staining). The "Color Threshold" also implemented by Gabriel Landini gives access to different color space representations of a true color image. This means that the user can select specific ranges of color tones (hue) and color saturation as well as their brightness (e.g., using the HSB color space). In this case, a limitation to specific color tones using the hue value slider is possible with a rather low bias (if I want only blue, I exclude all the other colors, which is easier to determine than an intensity cut-off value). Saturation is already more tricky to set with low-subjectiveness. For the thresholding of the brightness, the standard Auto Thresholds can be applied, thus reducing the bias slightly. </li>
<li> Similar to the method described above but more replicable, the image can be split into the respective channels of different color spaces (HSB, CIELAB,...) and those channels can then be automatically thresholded since all of them are based on a single intensity scale. It might already be sufficient to extract the desired information by thresholding only one of the color space channels. A combination of the extraction results from two or all three channels is also possible by using boolean functions (such as AND, OR, XOR) where applicable. This method is very effective with the right images (e.g., it can identify and separate overlapping cells).</li>
<li> <a href="Colour_Deconvolution" title="Colour Deconvolution">Color deconvolution</a> might also be a solution. In this method, the vector definition with individual staining components is an important step; it is nicely implemented in the tool and should be done for the sake of consistency and accuracy. A limitation inheres in using the definition by ROIs, which again introduces low <a href="Reproducibility" class="mw-redirect" title="Reproducibility">reproducibility</a> due to reliance on biased, user defined areas of "a color". This is a fast option but the problem here is that if this is done on the image which should be segmented the colors in a brightfield image exist usually in a mixed fashion meaning that the user defines already a mixture of colors to later separate exactly those. This may lead to inconsistent results even though it might visually look good in the first place.</li>
<li> Specific tools based on machine learning might be helpful. Here, the user is required to do some training on representative example images. This is achieved by selecting areas which should be assigned to the foreground or background, respectively. This is obviously also biased, with a good training (potentially by different experts) the feature extraction contains a lower bias, since the same trained classifier is applied to the different images. Relevant ImageJ plugins available are the <a href="SIOX:_Simple_Interactive_Object_Extraction" title="SIOX: Simple Interactive Object Extraction">SIOX: Simple Interactive Object Extraction</a> and the <a href="Trainable_Weka_Segmentation" title="Trainable Weka Segmentation">Trainable WEKA Segmentation</a>.</li>
<li> ...</li></ol>
<h1><span class="mw-headline" id="Image_analysis_principles">Image analysis principles</span></h1>
<h2><span class="mw-headline" id="Introduction_3">Introduction</span></h2>
<p>The following principles help with the analysis of processed images.
</p>
<!-- 
NewPP limit report
Cached time: 20200713073134
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.044 seconds
Real time usage: 0.043 seconds
Preprocessor visited node count: 457/1000000
Preprocessor generated node count: 864/1000000
Post‐expand include size: 2282/2097152 bytes
Template argument size: 318/2097152 bytes
Highest expansion depth: 6/40
Expensive parser function count: 0/3
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%   11.097      1 - -total
 96.02%   10.655      6 - Template:Bc
 24.88%    2.760      8 - Template:Arrow
-->
</div><div class="printfooter">
Retrieved from "<a dir="ltr" href="index.php?title=Principles&amp;oldid=41307">http://imagej.net/index.php?title=Principles&amp;oldid=41307</a>"</div>
							</div>

			<div id="footer">
				<p> This page was last modified on 24 January 2020, at 11:55.</p><ul><li><a href="ImageJ:Privacy_policy" title="ImageJ:Privacy policy">Privacy policy</a></li><li><a href="ImageJ:About" class="mw-redirect" title="ImageJ:About">About ImageJ</a></li><li><a href="Imprint" title="Imprint">Imprint</a></li><li><a href="index.php?title=Principles&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li></ul>			</div>

			<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="Special:Categories" title="Special:Categories">Category</a>: <ul><li><a href="Category:Tutorials" title="Category:Tutorials">Tutorials</a></li></ul></div></div>		</div>

		<div id="bottom-wrap">
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.loader.load(["ext.fancytree","ext.suckerfish","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","ext.SimpleTooltip"]);});</script><script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":269});});</script>
		</body>
		</html>
		